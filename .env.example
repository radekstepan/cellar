# LLM Provider Configuration
# Copy this file to .env and fill in your values

# OpenAI (GPT-4 for evaluations)
OPENAI_API_KEY=
OPENAI_MODEL=gpt-4-turbo

# Anthropic (Claude for evaluations)
ANTHROPIC_API_KEY=
ANTHROPIC_MODEL=claude-3-opus-20240229

# Local Ollama (for local models)
OLLAMA_URL=http://localhost:11434/api/generate
OLLAMA_MODEL=llama3

# Infisical (optional - if configured, these are pulled from Infisical)
# Leave blank here if using Infisical
# See .infisical.json.example for configuration
